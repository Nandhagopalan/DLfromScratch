{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Char Level Sequence_Modelling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RcRUeFKpbd9U",
        "colab_type": "code",
        "outputId": "e742d79d-dd8f-4746-cc27-b3f3aab673af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "23glWxSxfve-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Reading and processing text\n",
        "\n",
        "with open('Shakespeare.txt', 'r', encoding='utf-8') as f: \n",
        "    text=f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jV9LknR8fqAR",
        "colab_type": "code",
        "outputId": "f0f6f07e-a8c5-43af-963b-764a0b3aea13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "chars = set(text)\n",
        "print(len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "178707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zO4PifcmhUoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "char2int = {ch:i for i,ch in enumerate(chars)}\n",
        "int2char = dict(enumerate(chars))\n",
        "text_ints = np.array([char2int[ch] for ch in text], \n",
        "                     dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIt5DNV0h07J",
        "colab_type": "code",
        "outputId": "c1e3e1b8-eaa2-4bca-b809-43092ac78a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "def reshape_data(sequence, batch_size, num_steps):\n",
        "    mini_batch_length = batch_size * num_steps\n",
        "    num_batches = int(len(sequence) / mini_batch_length)\n",
        "    print(num_batches)\n",
        "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
        "        num_batches = num_batches - 1\n",
        "    \n",
        "    ## Truncate the sequence at the end to get rid of \n",
        "    ## remaining charcaters that do not make a full batch\n",
        "    x = sequence[0 : num_batches*mini_batch_length]\n",
        "    y = sequence[1 : num_batches*mini_batch_length + 1]\n",
        "    \n",
        "    ## Split x & y into a list batches of sequences: \n",
        "    x_batch_splits = np.split(x, batch_size)\n",
        "    y_batch_splits = np.split(y, batch_size)\n",
        " \n",
        "    \n",
        "    ## Stack the batches together\n",
        "    ## batch_size x mini_batch_length\n",
        "    x = np.stack(x_batch_splits)\n",
        "    y = np.stack(y_batch_splits)\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "## Testing:\n",
        "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
        "print(train_x.shape)\n",
        "print(train_x[0, :10])\n",
        "print(train_y[0, :10])\n",
        "print(''.join(int2char[i] for i in train_x[0, :50]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "279\n",
            "(64, 2790)\n",
            "[33 33 33 67 88 14 76 64 35 77]\n",
            "[33 33 67 88 14 76 64 35 77  2]\n",
            "***The Project Gutenberg's Etext of Shakespeare's \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ov5eIoj3iVDc",
        "colab_type": "code",
        "outputId": "9457a223-f1f3-4902-f4de-1ae550741c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "def create_batch_generator(data_x, data_y, num_steps):\n",
        "    batch_size, tot_batch_length = data_x.shape    \n",
        "    num_batches = int(tot_batch_length/num_steps)\n",
        "\n",
        "    for b in range(num_batches):\n",
        "        yield (data_x[:, b*num_steps: (b+1)*num_steps], \n",
        "               data_y[:, b*num_steps: (b+1)*num_steps])\n",
        "        \n",
        "bgen = create_batch_generator(train_x[:,:100], train_y[:,:100], 15)\n",
        "for b in bgen:\n",
        "    print(b[0].shape, b[1].shape, end='  ')\n",
        "    print(''.join(int2char[i] for i in b[0][0,:]).replace('\\n', '*'), '    ',\n",
        "          ''.join(int2char[i] for i in b[1][0,:]).replace('\\n', '*'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 15) (64, 15)  ***The Project       **The Project G\n",
            "(64, 15) (64, 15)  Gutenberg's Ete      utenberg's Etex\n",
            "(64, 15) (64, 15)  xt of Shakespea      t of Shakespear\n",
            "(64, 15) (64, 15)  re's First Foli      e's First Folio\n",
            "(64, 15) (64, 15)  o**************      ***************\n",
            "(64, 15) (64, 15)  ***********The       **********The T\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VEyBisL4hTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(object):\n",
        "  \n",
        "    def __init__(self, num_classes, batch_size=64, \n",
        "                 num_steps=100, lstm_size=128, \n",
        "                 num_layers=2, learning_rate=0.001, \n",
        "                 keep_prob=0.5, grad_clip=5, \n",
        "                 sampling=False):\n",
        "        self.num_classes = num_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.num_steps = num_steps\n",
        "        self.lstm_size = lstm_size\n",
        "        self.num_layers = num_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.keep_prob = keep_prob\n",
        "        self.grad_clip = grad_clip\n",
        "        \n",
        "        self.g = tf.Graph()\n",
        "        with self.g.as_default():\n",
        "            tf.set_random_seed(123)\n",
        "\n",
        "            self.build(sampling=sampling)\n",
        "            self.saver = tf.train.Saver()\n",
        "            self.init_op = tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "    def build(self, sampling):\n",
        "        if sampling == True:\n",
        "            batch_size, num_steps = 1, 1\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        "            num_steps = self.num_steps\n",
        "\n",
        "        tf_x = tf.placeholder(tf.int32, \n",
        "                              shape=[batch_size, num_steps], \n",
        "                              name='tf_x')\n",
        "        tf_y = tf.placeholder(tf.int32, \n",
        "                              shape=[batch_size, num_steps], \n",
        "                              name='tf_y')\n",
        "        tf_keepprob = tf.placeholder(tf.float32, \n",
        "                              name='tf_keepprob')\n",
        "\n",
        "        # One-hot encoding:\n",
        "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
        "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
        "        \n",
        "        ### Build the multi-layer RNN cells\n",
        "        cells = tf.contrib.rnn.MultiRNNCell(\n",
        "            [tf.contrib.rnn.DropoutWrapper(\n",
        "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size), \n",
        "                output_keep_prob=tf_keepprob) \n",
        "            for _ in range(self.num_layers)])\n",
        "        \n",
        "        ## Define the initial state\n",
        "        self.initial_state = cells.zero_state(\n",
        "                    batch_size, tf.float32)\n",
        "\n",
        "        ## Run each sequence step through the RNN \n",
        "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
        "                    cells, x_onehot, \n",
        "                    initial_state=self.initial_state)\n",
        "        \n",
        "        print('  << lstm_outputs  >>', lstm_outputs)\n",
        "\n",
        "        seq_output_reshaped = tf.reshape(\n",
        "                    lstm_outputs, \n",
        "                    shape=[-1, self.lstm_size],\n",
        "                    name='seq_output_reshaped')\n",
        "        print(\"reshaped output\",seq_output_reshaped)\n",
        "        \n",
        "        logits = tf.layers.dense(\n",
        "                    inputs=seq_output_reshaped, \n",
        "                    units=self.num_classes,\n",
        "                    activation=None,\n",
        "                    name='logits')\n",
        "\n",
        "        proba = tf.nn.softmax(\n",
        "                    logits, \n",
        "                    name='probabilities')\n",
        "        print(\"Prob\",proba)\n",
        "\n",
        "        y_reshaped = tf.reshape(\n",
        "                    y_onehot, \n",
        "                    shape=[-1, self.num_classes],\n",
        "                    name='y_reshaped')\n",
        "        \n",
        "        print(\"y\",y_reshaped)\n",
        "        cost = tf.reduce_mean(\n",
        "                    tf.nn.softmax_cross_entropy_with_logits(\n",
        "                        logits=logits, \n",
        "                        labels=y_reshaped),\n",
        "                        name='cost')\n",
        "        \n",
        "        # Gradient clipping to avoid \"exploding gradients\"\n",
        "        tvars = tf.trainable_variables()\n",
        "        grads, _ = tf.clip_by_global_norm(\n",
        "                    tf.gradients(cost, tvars), \n",
        "                    self.grad_clip)\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        train_op = optimizer.apply_gradients(\n",
        "                    zip(grads, tvars),\n",
        "                    name='train_op')\n",
        "        \n",
        "        \n",
        "    def train(self, train_x, train_y, \n",
        "          num_epochs, ckpt_dir='./model/'):\n",
        "        \n",
        "          ## Create the checkpoint directory\n",
        "          ## if does not exists\n",
        "        \n",
        "          if not os.path.exists(ckpt_dir):\n",
        "            os.mkdir(ckpt_dir)\n",
        "          \n",
        "          print(\"in training\")\n",
        "          with tf.Session(graph=self.g) as sess:\n",
        "            sess.run(self.init_op)\n",
        "\n",
        "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
        "            iterations = n_batches * num_epochs\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "\n",
        "                # Train network\n",
        "                new_state = sess.run(self.initial_state)\n",
        "                loss = 0\n",
        "                \n",
        "                ## Minibatch generator:\n",
        "                bgen = create_batch_generator(\n",
        "                        train_x, train_y, self.num_steps)\n",
        "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
        "                    \n",
        "                    iteration = epoch*n_batches + b\n",
        "                    \n",
        "                    feed = {'tf_x:0': batch_x,\n",
        "                            'tf_y:0': batch_y,\n",
        "                            'tf_keepprob:0': self.keep_prob,\n",
        "                            self.initial_state : new_state}\n",
        "                    batch_cost, _, new_state = sess.run(\n",
        "                            ['cost:0', 'train_op', \n",
        "                                self.final_state],\n",
        "                            feed_dict=feed)\n",
        "                    if iteration % 10 == 0:\n",
        "                        print('Epoch %d/%d Iteration %d'\n",
        "                              '| Training loss: %.4f' % (\n",
        "                              epoch + 1, num_epochs, \n",
        "                              iteration, batch_cost))\n",
        "\n",
        "                ## Save the trained model    \n",
        "                self.saver.save(\n",
        "                        sess, os.path.join(\n",
        "                            ckpt_dir, 'language_modeling.ckpt'))\n",
        "                \n",
        "        \n",
        "    def sample(self, output_length, ckpt_dir, starter_seq=\"The \"):\n",
        "          \n",
        "            observed_seq = [ch for ch in starter_seq]        \n",
        "            \n",
        "            with tf.Session(graph=self.g) as sess:\n",
        "              self.saver.restore(\n",
        "                sess, \n",
        "                tf.train.latest_checkpoint(ckpt_dir))\n",
        "            \n",
        "              ## 1: run the model using the starter sequence\n",
        "              new_state = sess.run(self.initial_state)\n",
        "              for ch in starter_seq:\n",
        "                x = np.zeros((1, 1))\n",
        "                x[0,0] = char2int[ch]\n",
        "                feed = {'tf_x:0': x,\n",
        "                        'tf_keepprob:0': 1.0,\n",
        "                        self.initial_state: new_state}\n",
        "                proba, new_state = sess.run(\n",
        "                        ['probabilities:0', self.final_state], \n",
        "                        feed_dict=feed)\n",
        "                print(\"prob\",proba)\n",
        "              ch_id = get_top_char(proba, len(chars))\n",
        "             \n",
        "              observed_seq.append(int2char[ch_id])\n",
        "              print(\"obsss\",observed_seq)     \n",
        "              ## 2: run the model using the updated observed_seq\n",
        "              for i in range(output_length):\n",
        "                x[0,0] = ch_id\n",
        "                feed = {'tf_x:0': x,\n",
        "                        'tf_keepprob:0': 1.0,\n",
        "                        self.initial_state: new_state}\n",
        "                proba, new_state = sess.run(\n",
        "                        ['probabilities:0', self.final_state], \n",
        "                        feed_dict=feed)\n",
        "\n",
        "                ch_id = get_top_char(proba, len(chars))\n",
        "                observed_seq.append(int2char[ch_id])\n",
        "\n",
        "              return ''.join(observed_seq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNldOU9Z6QYw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_top_char(probas, char_size, top_n=5):\n",
        "    p = np.squeeze(probas)\n",
        "    p[np.argsort(p)[:-top_n]] = 0.0\n",
        "    p = p / np.sum(p)\n",
        "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
        "    return ch_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qjEgVPoU6j4y",
        "colab_type": "code",
        "outputId": "6a8c91bc-0271-44d4-958c-11acc5778d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9316
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_steps = 100 \n",
        "train_x, train_y = reshape_data(text_ints, \n",
        "                                batch_size, \n",
        "                                num_steps)\n",
        "\n",
        "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
        "print(\"instantiated done\")\n",
        "rnn.train(train_x, train_y, \n",
        "          num_epochs=200,\n",
        "          ckpt_dir='./model-100/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
            "reshaped output Tensor(\"seq_output_reshaped:0\", shape=(6400, 128), dtype=float32)\n",
            "Prob Tensor(\"probabilities:0\", shape=(6400, 90), dtype=float32)\n",
            "y Tensor(\"y_reshaped:0\", shape=(6400, 90), dtype=float32)\n",
            "instantiated done\n",
            "in training\n",
            "Epoch 1/200 Iteration 10| Training loss: 3.8259\n",
            "Epoch 1/200 Iteration 20| Training loss: 3.4339\n",
            "Epoch 2/200 Iteration 30| Training loss: 3.3509\n",
            "Epoch 2/200 Iteration 40| Training loss: 3.3265\n",
            "Epoch 2/200 Iteration 50| Training loss: 3.3058\n",
            "Epoch 3/200 Iteration 60| Training loss: 3.3111\n",
            "Epoch 3/200 Iteration 70| Training loss: 3.2488\n",
            "Epoch 3/200 Iteration 80| Training loss: 3.3018\n",
            "Epoch 4/200 Iteration 90| Training loss: 3.2563\n",
            "Epoch 4/200 Iteration 100| Training loss: 3.2235\n",
            "Epoch 5/200 Iteration 110| Training loss: 3.2337\n",
            "Epoch 5/200 Iteration 120| Training loss: 3.2486\n",
            "Epoch 5/200 Iteration 130| Training loss: 3.2199\n",
            "Epoch 6/200 Iteration 140| Training loss: 3.2491\n",
            "Epoch 6/200 Iteration 150| Training loss: 3.1962\n",
            "Epoch 6/200 Iteration 160| Training loss: 3.2069\n",
            "Epoch 7/200 Iteration 170| Training loss: 3.1228\n",
            "Epoch 7/200 Iteration 180| Training loss: 3.1028\n",
            "Epoch 8/200 Iteration 190| Training loss: 3.1423\n",
            "Epoch 8/200 Iteration 200| Training loss: 3.0901\n",
            "Epoch 8/200 Iteration 210| Training loss: 3.0279\n",
            "Epoch 9/200 Iteration 220| Training loss: 2.9759\n",
            "Epoch 9/200 Iteration 230| Training loss: 2.9368\n",
            "Epoch 9/200 Iteration 240| Training loss: 2.9269\n",
            "Epoch 10/200 Iteration 250| Training loss: 2.8703\n",
            "Epoch 10/200 Iteration 260| Training loss: 2.8162\n",
            "Epoch 10/200 Iteration 270| Training loss: 2.8162\n",
            "Epoch 11/200 Iteration 280| Training loss: 2.7951\n",
            "Epoch 11/200 Iteration 290| Training loss: 2.7218\n",
            "Epoch 12/200 Iteration 300| Training loss: 2.6838\n",
            "Epoch 12/200 Iteration 310| Training loss: 2.6786\n",
            "Epoch 12/200 Iteration 320| Training loss: 2.6946\n",
            "Epoch 13/200 Iteration 330| Training loss: 2.6317\n",
            "Epoch 13/200 Iteration 340| Training loss: 2.5915\n",
            "Epoch 13/200 Iteration 350| Training loss: 2.6064\n",
            "Epoch 14/200 Iteration 360| Training loss: 2.6007\n",
            "Epoch 14/200 Iteration 370| Training loss: 2.5455\n",
            "Epoch 15/200 Iteration 380| Training loss: 2.5572\n",
            "Epoch 15/200 Iteration 390| Training loss: 2.5697\n",
            "Epoch 15/200 Iteration 400| Training loss: 2.5440\n",
            "Epoch 16/200 Iteration 410| Training loss: 2.5428\n",
            "Epoch 16/200 Iteration 420| Training loss: 2.4965\n",
            "Epoch 16/200 Iteration 430| Training loss: 2.5065\n",
            "Epoch 17/200 Iteration 440| Training loss: 2.4767\n",
            "Epoch 17/200 Iteration 450| Training loss: 2.4878\n",
            "Epoch 18/200 Iteration 460| Training loss: 2.5403\n",
            "Epoch 18/200 Iteration 470| Training loss: 2.5180\n",
            "Epoch 18/200 Iteration 480| Training loss: 2.5093\n",
            "Epoch 19/200 Iteration 490| Training loss: 2.4712\n",
            "Epoch 19/200 Iteration 500| Training loss: 2.4427\n",
            "Epoch 19/200 Iteration 510| Training loss: 2.4541\n",
            "Epoch 20/200 Iteration 520| Training loss: 2.4411\n",
            "Epoch 20/200 Iteration 530| Training loss: 2.4109\n",
            "Epoch 20/200 Iteration 540| Training loss: 2.4468\n",
            "Epoch 21/200 Iteration 550| Training loss: 2.4436\n",
            "Epoch 21/200 Iteration 560| Training loss: 2.4130\n",
            "Epoch 22/200 Iteration 570| Training loss: 2.3983\n",
            "Epoch 22/200 Iteration 580| Training loss: 2.3770\n",
            "Epoch 22/200 Iteration 590| Training loss: 2.4391\n",
            "Epoch 23/200 Iteration 600| Training loss: 2.3741\n",
            "Epoch 23/200 Iteration 610| Training loss: 2.3616\n",
            "Epoch 23/200 Iteration 620| Training loss: 2.3793\n",
            "Epoch 24/200 Iteration 630| Training loss: 2.3675\n",
            "Epoch 24/200 Iteration 640| Training loss: 2.3435\n",
            "Epoch 25/200 Iteration 650| Training loss: 2.3698\n",
            "Epoch 25/200 Iteration 660| Training loss: 2.3703\n",
            "Epoch 25/200 Iteration 670| Training loss: 2.3727\n",
            "Epoch 26/200 Iteration 680| Training loss: 2.3671\n",
            "Epoch 26/200 Iteration 690| Training loss: 2.3202\n",
            "Epoch 26/200 Iteration 700| Training loss: 2.3429\n",
            "Epoch 27/200 Iteration 710| Training loss: 2.3254\n",
            "Epoch 27/200 Iteration 720| Training loss: 2.3331\n",
            "Epoch 28/200 Iteration 730| Training loss: 2.3750\n",
            "Epoch 28/200 Iteration 740| Training loss: 2.3451\n",
            "Epoch 28/200 Iteration 750| Training loss: 2.3561\n",
            "Epoch 29/200 Iteration 760| Training loss: 2.3253\n",
            "Epoch 29/200 Iteration 770| Training loss: 2.2980\n",
            "Epoch 29/200 Iteration 780| Training loss: 2.3213\n",
            "Epoch 30/200 Iteration 790| Training loss: 2.3109\n",
            "Epoch 30/200 Iteration 800| Training loss: 2.2868\n",
            "Epoch 30/200 Iteration 810| Training loss: 2.3153\n",
            "Epoch 31/200 Iteration 820| Training loss: 2.3109\n",
            "Epoch 31/200 Iteration 830| Training loss: 2.2904\n",
            "Epoch 32/200 Iteration 840| Training loss: 2.2694\n",
            "Epoch 32/200 Iteration 850| Training loss: 2.2472\n",
            "Epoch 32/200 Iteration 860| Training loss: 2.2969\n",
            "Epoch 33/200 Iteration 870| Training loss: 2.2549\n",
            "Epoch 33/200 Iteration 880| Training loss: 2.2446\n",
            "Epoch 33/200 Iteration 890| Training loss: 2.2611\n",
            "Epoch 34/200 Iteration 900| Training loss: 2.2478\n",
            "Epoch 34/200 Iteration 910| Training loss: 2.2350\n",
            "Epoch 35/200 Iteration 920| Training loss: 2.2533\n",
            "Epoch 35/200 Iteration 930| Training loss: 2.2642\n",
            "Epoch 35/200 Iteration 940| Training loss: 2.2623\n",
            "Epoch 36/200 Iteration 950| Training loss: 2.2560\n",
            "Epoch 36/200 Iteration 960| Training loss: 2.2279\n",
            "Epoch 36/200 Iteration 970| Training loss: 2.2442\n",
            "Epoch 37/200 Iteration 980| Training loss: 2.1985\n",
            "Epoch 37/200 Iteration 990| Training loss: 2.2370\n",
            "Epoch 38/200 Iteration 1000| Training loss: 2.2649\n",
            "Epoch 38/200 Iteration 1010| Training loss: 2.2597\n",
            "Epoch 38/200 Iteration 1020| Training loss: 2.2622\n",
            "Epoch 39/200 Iteration 1030| Training loss: 2.2303\n",
            "Epoch 39/200 Iteration 1040| Training loss: 2.1952\n",
            "Epoch 39/200 Iteration 1050| Training loss: 2.2218\n",
            "Epoch 40/200 Iteration 1060| Training loss: 2.2209\n",
            "Epoch 40/200 Iteration 1070| Training loss: 2.1887\n",
            "Epoch 40/200 Iteration 1080| Training loss: 2.2243\n",
            "Epoch 41/200 Iteration 1090| Training loss: 2.2089\n",
            "Epoch 41/200 Iteration 1100| Training loss: 2.1987\n",
            "Epoch 42/200 Iteration 1110| Training loss: 2.1750\n",
            "Epoch 42/200 Iteration 1120| Training loss: 2.1638\n",
            "Epoch 42/200 Iteration 1130| Training loss: 2.2075\n",
            "Epoch 43/200 Iteration 1140| Training loss: 2.1586\n",
            "Epoch 43/200 Iteration 1150| Training loss: 2.1530\n",
            "Epoch 43/200 Iteration 1160| Training loss: 2.1755\n",
            "Epoch 44/200 Iteration 1170| Training loss: 2.1631\n",
            "Epoch 44/200 Iteration 1180| Training loss: 2.1553\n",
            "Epoch 45/200 Iteration 1190| Training loss: 2.1728\n",
            "Epoch 45/200 Iteration 1200| Training loss: 2.1630\n",
            "Epoch 45/200 Iteration 1210| Training loss: 2.1797\n",
            "Epoch 46/200 Iteration 1220| Training loss: 2.1756\n",
            "Epoch 46/200 Iteration 1230| Training loss: 2.1310\n",
            "Epoch 46/200 Iteration 1240| Training loss: 2.1512\n",
            "Epoch 47/200 Iteration 1250| Training loss: 2.1144\n",
            "Epoch 47/200 Iteration 1260| Training loss: 2.1605\n",
            "Epoch 48/200 Iteration 1270| Training loss: 2.1827\n",
            "Epoch 48/200 Iteration 1280| Training loss: 2.1812\n",
            "Epoch 48/200 Iteration 1290| Training loss: 2.1820\n",
            "Epoch 49/200 Iteration 1300| Training loss: 2.1378\n",
            "Epoch 49/200 Iteration 1310| Training loss: 2.1201\n",
            "Epoch 49/200 Iteration 1320| Training loss: 2.1362\n",
            "Epoch 50/200 Iteration 1330| Training loss: 2.1503\n",
            "Epoch 50/200 Iteration 1340| Training loss: 2.1089\n",
            "Epoch 50/200 Iteration 1350| Training loss: 2.1518\n",
            "Epoch 51/200 Iteration 1360| Training loss: 2.1435\n",
            "Epoch 51/200 Iteration 1370| Training loss: 2.1365\n",
            "Epoch 52/200 Iteration 1380| Training loss: 2.0916\n",
            "Epoch 52/200 Iteration 1390| Training loss: 2.0800\n",
            "Epoch 52/200 Iteration 1400| Training loss: 2.1311\n",
            "Epoch 53/200 Iteration 1410| Training loss: 2.0793\n",
            "Epoch 53/200 Iteration 1420| Training loss: 2.0950\n",
            "Epoch 53/200 Iteration 1430| Training loss: 2.0848\n",
            "Epoch 54/200 Iteration 1440| Training loss: 2.1033\n",
            "Epoch 54/200 Iteration 1450| Training loss: 2.0933\n",
            "Epoch 55/200 Iteration 1460| Training loss: 2.1114\n",
            "Epoch 55/200 Iteration 1470| Training loss: 2.1058\n",
            "Epoch 55/200 Iteration 1480| Training loss: 2.1232\n",
            "Epoch 56/200 Iteration 1490| Training loss: 2.1070\n",
            "Epoch 56/200 Iteration 1500| Training loss: 2.0832\n",
            "Epoch 56/200 Iteration 1510| Training loss: 2.1056\n",
            "Epoch 57/200 Iteration 1520| Training loss: 2.0564\n",
            "Epoch 57/200 Iteration 1530| Training loss: 2.0956\n",
            "Epoch 58/200 Iteration 1540| Training loss: 2.1211\n",
            "Epoch 58/200 Iteration 1550| Training loss: 2.1065\n",
            "Epoch 58/200 Iteration 1560| Training loss: 2.1236\n",
            "Epoch 59/200 Iteration 1570| Training loss: 2.0686\n",
            "Epoch 59/200 Iteration 1580| Training loss: 2.0513\n",
            "Epoch 59/200 Iteration 1590| Training loss: 2.0764\n",
            "Epoch 60/200 Iteration 1600| Training loss: 2.0793\n",
            "Epoch 60/200 Iteration 1610| Training loss: 2.0424\n",
            "Epoch 60/200 Iteration 1620| Training loss: 2.0882\n",
            "Epoch 61/200 Iteration 1630| Training loss: 2.0769\n",
            "Epoch 61/200 Iteration 1640| Training loss: 2.0723\n",
            "Epoch 62/200 Iteration 1650| Training loss: 2.0392\n",
            "Epoch 62/200 Iteration 1660| Training loss: 2.0258\n",
            "Epoch 62/200 Iteration 1670| Training loss: 2.0807\n",
            "Epoch 63/200 Iteration 1680| Training loss: 2.0244\n",
            "Epoch 63/200 Iteration 1690| Training loss: 2.0242\n",
            "Epoch 63/200 Iteration 1700| Training loss: 2.0439\n",
            "Epoch 64/200 Iteration 1710| Training loss: 2.0298\n",
            "Epoch 64/200 Iteration 1720| Training loss: 2.0261\n",
            "Epoch 65/200 Iteration 1730| Training loss: 2.0606\n",
            "Epoch 65/200 Iteration 1740| Training loss: 2.0482\n",
            "Epoch 65/200 Iteration 1750| Training loss: 2.0631\n",
            "Epoch 66/200 Iteration 1760| Training loss: 2.0437\n",
            "Epoch 66/200 Iteration 1770| Training loss: 2.0143\n",
            "Epoch 66/200 Iteration 1780| Training loss: 2.0534\n",
            "Epoch 67/200 Iteration 1790| Training loss: 2.0113\n",
            "Epoch 67/200 Iteration 1800| Training loss: 2.0414\n",
            "Epoch 68/200 Iteration 1810| Training loss: 2.0779\n",
            "Epoch 68/200 Iteration 1820| Training loss: 2.0403\n",
            "Epoch 68/200 Iteration 1830| Training loss: 2.0839\n",
            "Epoch 69/200 Iteration 1840| Training loss: 2.0290\n",
            "Epoch 69/200 Iteration 1850| Training loss: 2.0055\n",
            "Epoch 69/200 Iteration 1860| Training loss: 2.0349\n",
            "Epoch 70/200 Iteration 1870| Training loss: 2.0219\n",
            "Epoch 70/200 Iteration 1880| Training loss: 1.9943\n",
            "Epoch 70/200 Iteration 1890| Training loss: 2.0492\n",
            "Epoch 71/200 Iteration 1900| Training loss: 2.0130\n",
            "Epoch 71/200 Iteration 1910| Training loss: 2.0193\n",
            "Epoch 72/200 Iteration 1920| Training loss: 1.9918\n",
            "Epoch 72/200 Iteration 1930| Training loss: 1.9783\n",
            "Epoch 72/200 Iteration 1940| Training loss: 2.0280\n",
            "Epoch 73/200 Iteration 1950| Training loss: 1.9712\n",
            "Epoch 73/200 Iteration 1960| Training loss: 1.9769\n",
            "Epoch 73/200 Iteration 1970| Training loss: 1.9980\n",
            "Epoch 74/200 Iteration 1980| Training loss: 1.9951\n",
            "Epoch 74/200 Iteration 1990| Training loss: 1.9801\n",
            "Epoch 75/200 Iteration 2000| Training loss: 2.0086\n",
            "Epoch 75/200 Iteration 2010| Training loss: 1.9974\n",
            "Epoch 75/200 Iteration 2020| Training loss: 2.0213\n",
            "Epoch 76/200 Iteration 2030| Training loss: 2.0054\n",
            "Epoch 76/200 Iteration 2040| Training loss: 1.9808\n",
            "Epoch 76/200 Iteration 2050| Training loss: 2.0024\n",
            "Epoch 77/200 Iteration 2060| Training loss: 1.9682\n",
            "Epoch 77/200 Iteration 2070| Training loss: 1.9886\n",
            "Epoch 78/200 Iteration 2080| Training loss: 2.0189\n",
            "Epoch 78/200 Iteration 2090| Training loss: 1.9893\n",
            "Epoch 78/200 Iteration 2100| Training loss: 2.0205\n",
            "Epoch 79/200 Iteration 2110| Training loss: 1.9833\n",
            "Epoch 79/200 Iteration 2120| Training loss: 1.9473\n",
            "Epoch 79/200 Iteration 2130| Training loss: 1.9868\n",
            "Epoch 80/200 Iteration 2140| Training loss: 1.9891\n",
            "Epoch 80/200 Iteration 2150| Training loss: 1.9589\n",
            "Epoch 80/200 Iteration 2160| Training loss: 1.9991\n",
            "Epoch 81/200 Iteration 2170| Training loss: 1.9782\n",
            "Epoch 81/200 Iteration 2180| Training loss: 1.9771\n",
            "Epoch 82/200 Iteration 2190| Training loss: 1.9556\n",
            "Epoch 82/200 Iteration 2200| Training loss: 1.9412\n",
            "Epoch 82/200 Iteration 2210| Training loss: 1.9968\n",
            "Epoch 83/200 Iteration 2220| Training loss: 1.9358\n",
            "Epoch 83/200 Iteration 2230| Training loss: 1.9459\n",
            "Epoch 83/200 Iteration 2240| Training loss: 1.9573\n",
            "Epoch 84/200 Iteration 2250| Training loss: 1.9450\n",
            "Epoch 84/200 Iteration 2260| Training loss: 1.9647\n",
            "Epoch 85/200 Iteration 2270| Training loss: 1.9706\n",
            "Epoch 85/200 Iteration 2280| Training loss: 1.9612\n",
            "Epoch 85/200 Iteration 2290| Training loss: 1.9793\n",
            "Epoch 86/200 Iteration 2300| Training loss: 1.9779\n",
            "Epoch 86/200 Iteration 2310| Training loss: 1.9458\n",
            "Epoch 86/200 Iteration 2320| Training loss: 1.9732\n",
            "Epoch 87/200 Iteration 2330| Training loss: 1.9200\n",
            "Epoch 87/200 Iteration 2340| Training loss: 1.9593\n",
            "Epoch 88/200 Iteration 2350| Training loss: 1.9868\n",
            "Epoch 88/200 Iteration 2360| Training loss: 1.9628\n",
            "Epoch 88/200 Iteration 2370| Training loss: 1.9926\n",
            "Epoch 89/200 Iteration 2380| Training loss: 1.9451\n",
            "Epoch 89/200 Iteration 2390| Training loss: 1.9282\n",
            "Epoch 89/200 Iteration 2400| Training loss: 1.9410\n",
            "Epoch 90/200 Iteration 2410| Training loss: 1.9499\n",
            "Epoch 90/200 Iteration 2420| Training loss: 1.9251\n",
            "Epoch 90/200 Iteration 2430| Training loss: 1.9813\n",
            "Epoch 91/200 Iteration 2440| Training loss: 1.9398\n",
            "Epoch 91/200 Iteration 2450| Training loss: 1.9357\n",
            "Epoch 92/200 Iteration 2460| Training loss: 1.9320\n",
            "Epoch 92/200 Iteration 2470| Training loss: 1.9123\n",
            "Epoch 92/200 Iteration 2480| Training loss: 1.9574\n",
            "Epoch 93/200 Iteration 2490| Training loss: 1.8927\n",
            "Epoch 93/200 Iteration 2500| Training loss: 1.9018\n",
            "Epoch 93/200 Iteration 2510| Training loss: 1.9144\n",
            "Epoch 94/200 Iteration 2520| Training loss: 1.9254\n",
            "Epoch 94/200 Iteration 2530| Training loss: 1.9254\n",
            "Epoch 95/200 Iteration 2540| Training loss: 1.9446\n",
            "Epoch 95/200 Iteration 2550| Training loss: 1.9279\n",
            "Epoch 95/200 Iteration 2560| Training loss: 1.9390\n",
            "Epoch 96/200 Iteration 2570| Training loss: 1.9322\n",
            "Epoch 96/200 Iteration 2580| Training loss: 1.9047\n",
            "Epoch 96/200 Iteration 2590| Training loss: 1.9338\n",
            "Epoch 97/200 Iteration 2600| Training loss: 1.8890\n",
            "Epoch 97/200 Iteration 2610| Training loss: 1.9261\n",
            "Epoch 98/200 Iteration 2620| Training loss: 1.9523\n",
            "Epoch 98/200 Iteration 2630| Training loss: 1.9195\n",
            "Epoch 98/200 Iteration 2640| Training loss: 1.9645\n",
            "Epoch 99/200 Iteration 2650| Training loss: 1.9182\n",
            "Epoch 99/200 Iteration 2660| Training loss: 1.9006\n",
            "Epoch 99/200 Iteration 2670| Training loss: 1.9177\n",
            "Epoch 100/200 Iteration 2680| Training loss: 1.9350\n",
            "Epoch 100/200 Iteration 2690| Training loss: 1.8992\n",
            "Epoch 100/200 Iteration 2700| Training loss: 1.9382\n",
            "Epoch 101/200 Iteration 2710| Training loss: 1.9132\n",
            "Epoch 101/200 Iteration 2720| Training loss: 1.9127\n",
            "Epoch 102/200 Iteration 2730| Training loss: 1.8840\n",
            "Epoch 102/200 Iteration 2740| Training loss: 1.8726\n",
            "Epoch 102/200 Iteration 2750| Training loss: 1.9233\n",
            "Epoch 103/200 Iteration 2760| Training loss: 1.8615\n",
            "Epoch 103/200 Iteration 2770| Training loss: 1.8814\n",
            "Epoch 103/200 Iteration 2780| Training loss: 1.8929\n",
            "Epoch 104/200 Iteration 2790| Training loss: 1.9050\n",
            "Epoch 104/200 Iteration 2800| Training loss: 1.8908\n",
            "Epoch 105/200 Iteration 2810| Training loss: 1.9118\n",
            "Epoch 105/200 Iteration 2820| Training loss: 1.8985\n",
            "Epoch 105/200 Iteration 2830| Training loss: 1.9073\n",
            "Epoch 106/200 Iteration 2840| Training loss: 1.8808\n",
            "Epoch 106/200 Iteration 2850| Training loss: 1.8750\n",
            "Epoch 106/200 Iteration 2860| Training loss: 1.9100\n",
            "Epoch 107/200 Iteration 2870| Training loss: 1.8799\n",
            "Epoch 107/200 Iteration 2880| Training loss: 1.9074\n",
            "Epoch 108/200 Iteration 2890| Training loss: 1.9195\n",
            "Epoch 108/200 Iteration 2900| Training loss: 1.8848\n",
            "Epoch 108/200 Iteration 2910| Training loss: 1.9442\n",
            "Epoch 109/200 Iteration 2920| Training loss: 1.8742\n",
            "Epoch 109/200 Iteration 2930| Training loss: 1.8532\n",
            "Epoch 109/200 Iteration 2940| Training loss: 1.8896\n",
            "Epoch 110/200 Iteration 2950| Training loss: 1.8844\n",
            "Epoch 110/200 Iteration 2960| Training loss: 1.8620\n",
            "Epoch 110/200 Iteration 2970| Training loss: 1.9219\n",
            "Epoch 111/200 Iteration 2980| Training loss: 1.8848\n",
            "Epoch 111/200 Iteration 2990| Training loss: 1.9082\n",
            "Epoch 112/200 Iteration 3000| Training loss: 1.8713\n",
            "Epoch 112/200 Iteration 3010| Training loss: 1.8446\n",
            "Epoch 112/200 Iteration 3020| Training loss: 1.9126\n",
            "Epoch 113/200 Iteration 3030| Training loss: 1.8381\n",
            "Epoch 113/200 Iteration 3040| Training loss: 1.8501\n",
            "Epoch 113/200 Iteration 3050| Training loss: 1.8719\n",
            "Epoch 114/200 Iteration 3060| Training loss: 1.8540\n",
            "Epoch 114/200 Iteration 3070| Training loss: 1.8622\n",
            "Epoch 115/200 Iteration 3080| Training loss: 1.8803\n",
            "Epoch 115/200 Iteration 3090| Training loss: 1.8671\n",
            "Epoch 115/200 Iteration 3100| Training loss: 1.8918\n",
            "Epoch 116/200 Iteration 3110| Training loss: 1.8565\n",
            "Epoch 116/200 Iteration 3120| Training loss: 1.8458\n",
            "Epoch 116/200 Iteration 3130| Training loss: 1.8856\n",
            "Epoch 117/200 Iteration 3140| Training loss: 1.8413\n",
            "Epoch 117/200 Iteration 3150| Training loss: 1.8810\n",
            "Epoch 118/200 Iteration 3160| Training loss: 1.9111\n",
            "Epoch 118/200 Iteration 3170| Training loss: 1.8615\n",
            "Epoch 118/200 Iteration 3180| Training loss: 1.9107\n",
            "Epoch 119/200 Iteration 3190| Training loss: 1.8465\n",
            "Epoch 119/200 Iteration 3200| Training loss: 1.8381\n",
            "Epoch 119/200 Iteration 3210| Training loss: 1.8550\n",
            "Epoch 120/200 Iteration 3220| Training loss: 1.8700\n",
            "Epoch 120/200 Iteration 3230| Training loss: 1.8423\n",
            "Epoch 120/200 Iteration 3240| Training loss: 1.8959\n",
            "Epoch 121/200 Iteration 3250| Training loss: 1.8514\n",
            "Epoch 121/200 Iteration 3260| Training loss: 1.8693\n",
            "Epoch 122/200 Iteration 3270| Training loss: 1.8338\n",
            "Epoch 122/200 Iteration 3280| Training loss: 1.8248\n",
            "Epoch 122/200 Iteration 3290| Training loss: 1.8895\n",
            "Epoch 123/200 Iteration 3300| Training loss: 1.8057\n",
            "Epoch 123/200 Iteration 3310| Training loss: 1.8305\n",
            "Epoch 123/200 Iteration 3320| Training loss: 1.8433\n",
            "Epoch 124/200 Iteration 3330| Training loss: 1.8345\n",
            "Epoch 124/200 Iteration 3340| Training loss: 1.8328\n",
            "Epoch 125/200 Iteration 3350| Training loss: 1.8600\n",
            "Epoch 125/200 Iteration 3360| Training loss: 1.8434\n",
            "Epoch 125/200 Iteration 3370| Training loss: 1.8632\n",
            "Epoch 126/200 Iteration 3380| Training loss: 1.8339\n",
            "Epoch 126/200 Iteration 3390| Training loss: 1.8132\n",
            "Epoch 126/200 Iteration 3400| Training loss: 1.8661\n",
            "Epoch 127/200 Iteration 3410| Training loss: 1.8246\n",
            "Epoch 127/200 Iteration 3420| Training loss: 1.8520\n",
            "Epoch 128/200 Iteration 3430| Training loss: 1.8921\n",
            "Epoch 128/200 Iteration 3440| Training loss: 1.8450\n",
            "Epoch 128/200 Iteration 3450| Training loss: 1.8942\n",
            "Epoch 129/200 Iteration 3460| Training loss: 1.8314\n",
            "Epoch 129/200 Iteration 3470| Training loss: 1.8123\n",
            "Epoch 129/200 Iteration 3480| Training loss: 1.8463\n",
            "Epoch 130/200 Iteration 3490| Training loss: 1.8437\n",
            "Epoch 130/200 Iteration 3500| Training loss: 1.8074\n",
            "Epoch 130/200 Iteration 3510| Training loss: 1.8579\n",
            "Epoch 131/200 Iteration 3520| Training loss: 1.8334\n",
            "Epoch 131/200 Iteration 3530| Training loss: 1.8485\n",
            "Epoch 132/200 Iteration 3540| Training loss: 1.8115\n",
            "Epoch 132/200 Iteration 3550| Training loss: 1.8168\n",
            "Epoch 132/200 Iteration 3560| Training loss: 1.8594\n",
            "Epoch 133/200 Iteration 3570| Training loss: 1.7851\n",
            "Epoch 133/200 Iteration 3580| Training loss: 1.8068\n",
            "Epoch 133/200 Iteration 3590| Training loss: 1.8151\n",
            "Epoch 134/200 Iteration 3600| Training loss: 1.8129\n",
            "Epoch 134/200 Iteration 3610| Training loss: 1.8087\n",
            "Epoch 135/200 Iteration 3620| Training loss: 1.8366\n",
            "Epoch 135/200 Iteration 3630| Training loss: 1.8218\n",
            "Epoch 135/200 Iteration 3640| Training loss: 1.8589\n",
            "Epoch 136/200 Iteration 3650| Training loss: 1.8119\n",
            "Epoch 136/200 Iteration 3660| Training loss: 1.8015\n",
            "Epoch 136/200 Iteration 3670| Training loss: 1.8437\n",
            "Epoch 137/200 Iteration 3680| Training loss: 1.8012\n",
            "Epoch 137/200 Iteration 3690| Training loss: 1.8387\n",
            "Epoch 138/200 Iteration 3700| Training loss: 1.8580\n",
            "Epoch 138/200 Iteration 3710| Training loss: 1.8281\n",
            "Epoch 138/200 Iteration 3720| Training loss: 1.8659\n",
            "Epoch 139/200 Iteration 3730| Training loss: 1.8161\n",
            "Epoch 139/200 Iteration 3740| Training loss: 1.7977\n",
            "Epoch 139/200 Iteration 3750| Training loss: 1.8138\n",
            "Epoch 140/200 Iteration 3760| Training loss: 1.8375\n",
            "Epoch 140/200 Iteration 3770| Training loss: 1.7940\n",
            "Epoch 140/200 Iteration 3780| Training loss: 1.8411\n",
            "Epoch 141/200 Iteration 3790| Training loss: 1.8091\n",
            "Epoch 141/200 Iteration 3800| Training loss: 1.8364\n",
            "Epoch 142/200 Iteration 3810| Training loss: 1.7804\n",
            "Epoch 142/200 Iteration 3820| Training loss: 1.7849\n",
            "Epoch 142/200 Iteration 3830| Training loss: 1.8337\n",
            "Epoch 143/200 Iteration 3840| Training loss: 1.7515\n",
            "Epoch 143/200 Iteration 3850| Training loss: 1.7920\n",
            "Epoch 143/200 Iteration 3860| Training loss: 1.7908\n",
            "Epoch 144/200 Iteration 3870| Training loss: 1.8004\n",
            "Epoch 144/200 Iteration 3880| Training loss: 1.7986\n",
            "Epoch 145/200 Iteration 3890| Training loss: 1.8189\n",
            "Epoch 145/200 Iteration 3900| Training loss: 1.8054\n",
            "Epoch 145/200 Iteration 3910| Training loss: 1.8387\n",
            "Epoch 146/200 Iteration 3920| Training loss: 1.8018\n",
            "Epoch 146/200 Iteration 3930| Training loss: 1.7964\n",
            "Epoch 146/200 Iteration 3940| Training loss: 1.8197\n",
            "Epoch 147/200 Iteration 3950| Training loss: 1.7839\n",
            "Epoch 147/200 Iteration 3960| Training loss: 1.8101\n",
            "Epoch 148/200 Iteration 3970| Training loss: 1.8336\n",
            "Epoch 148/200 Iteration 3980| Training loss: 1.7968\n",
            "Epoch 148/200 Iteration 3990| Training loss: 1.8633\n",
            "Epoch 149/200 Iteration 4000| Training loss: 1.7823\n",
            "Epoch 149/200 Iteration 4010| Training loss: 1.7818\n",
            "Epoch 149/200 Iteration 4020| Training loss: 1.8014\n",
            "Epoch 150/200 Iteration 4030| Training loss: 1.8064\n",
            "Epoch 150/200 Iteration 4040| Training loss: 1.7805\n",
            "Epoch 150/200 Iteration 4050| Training loss: 1.8328\n",
            "Epoch 151/200 Iteration 4060| Training loss: 1.7865\n",
            "Epoch 151/200 Iteration 4070| Training loss: 1.8034\n",
            "Epoch 152/200 Iteration 4080| Training loss: 1.7721\n",
            "Epoch 152/200 Iteration 4090| Training loss: 1.7720\n",
            "Epoch 152/200 Iteration 4100| Training loss: 1.8273\n",
            "Epoch 153/200 Iteration 4110| Training loss: 1.7501\n",
            "Epoch 153/200 Iteration 4120| Training loss: 1.7769\n",
            "Epoch 153/200 Iteration 4130| Training loss: 1.7891\n",
            "Epoch 154/200 Iteration 4140| Training loss: 1.7727\n",
            "Epoch 154/200 Iteration 4150| Training loss: 1.7885\n",
            "Epoch 155/200 Iteration 4160| Training loss: 1.8110\n",
            "Epoch 155/200 Iteration 4170| Training loss: 1.7859\n",
            "Epoch 155/200 Iteration 4180| Training loss: 1.8137\n",
            "Epoch 156/200 Iteration 4190| Training loss: 1.7674\n",
            "Epoch 156/200 Iteration 4200| Training loss: 1.7687\n",
            "Epoch 156/200 Iteration 4210| Training loss: 1.8057\n",
            "Epoch 157/200 Iteration 4220| Training loss: 1.7683\n",
            "Epoch 157/200 Iteration 4230| Training loss: 1.7981\n",
            "Epoch 158/200 Iteration 4240| Training loss: 1.8328\n",
            "Epoch 158/200 Iteration 4250| Training loss: 1.7930\n",
            "Epoch 158/200 Iteration 4260| Training loss: 1.8339\n",
            "Epoch 159/200 Iteration 4270| Training loss: 1.7647\n",
            "Epoch 159/200 Iteration 4280| Training loss: 1.7485\n",
            "Epoch 159/200 Iteration 4290| Training loss: 1.7722\n",
            "Epoch 160/200 Iteration 4300| Training loss: 1.7806\n",
            "Epoch 160/200 Iteration 4310| Training loss: 1.7658\n",
            "Epoch 160/200 Iteration 4320| Training loss: 1.8111\n",
            "Epoch 161/200 Iteration 4330| Training loss: 1.7772\n",
            "Epoch 161/200 Iteration 4340| Training loss: 1.8264\n",
            "Epoch 162/200 Iteration 4350| Training loss: 1.7565\n",
            "Epoch 162/200 Iteration 4360| Training loss: 1.7412\n",
            "Epoch 162/200 Iteration 4370| Training loss: 1.8122\n",
            "Epoch 163/200 Iteration 4380| Training loss: 1.7427\n",
            "Epoch 163/200 Iteration 4390| Training loss: 1.7674\n",
            "Epoch 163/200 Iteration 4400| Training loss: 1.7745\n",
            "Epoch 164/200 Iteration 4410| Training loss: 1.7702\n",
            "Epoch 164/200 Iteration 4420| Training loss: 1.7629\n",
            "Epoch 165/200 Iteration 4430| Training loss: 1.7845\n",
            "Epoch 165/200 Iteration 4440| Training loss: 1.7507\n",
            "Epoch 165/200 Iteration 4450| Training loss: 1.7925\n",
            "Epoch 166/200 Iteration 4460| Training loss: 1.7585\n",
            "Epoch 166/200 Iteration 4470| Training loss: 1.7388\n",
            "Epoch 166/200 Iteration 4480| Training loss: 1.8084\n",
            "Epoch 167/200 Iteration 4490| Training loss: 1.7466\n",
            "Epoch 167/200 Iteration 4500| Training loss: 1.7884\n",
            "Epoch 168/200 Iteration 4510| Training loss: 1.8116\n",
            "Epoch 168/200 Iteration 4520| Training loss: 1.7687\n",
            "Epoch 168/200 Iteration 4530| Training loss: 1.8143\n",
            "Epoch 169/200 Iteration 4540| Training loss: 1.7581\n",
            "Epoch 169/200 Iteration 4550| Training loss: 1.7431\n",
            "Epoch 169/200 Iteration 4560| Training loss: 1.7774\n",
            "Epoch 170/200 Iteration 4570| Training loss: 1.7812\n",
            "Epoch 170/200 Iteration 4580| Training loss: 1.7380\n",
            "Epoch 170/200 Iteration 4590| Training loss: 1.7958\n",
            "Epoch 171/200 Iteration 4600| Training loss: 1.7355\n",
            "Epoch 171/200 Iteration 4610| Training loss: 1.7788\n",
            "Epoch 172/200 Iteration 4620| Training loss: 1.7427\n",
            "Epoch 172/200 Iteration 4630| Training loss: 1.7271\n",
            "Epoch 172/200 Iteration 4640| Training loss: 1.7866\n",
            "Epoch 173/200 Iteration 4650| Training loss: 1.6984\n",
            "Epoch 173/200 Iteration 4660| Training loss: 1.7309\n",
            "Epoch 173/200 Iteration 4670| Training loss: 1.7361\n",
            "Epoch 174/200 Iteration 4680| Training loss: 1.7450\n",
            "Epoch 174/200 Iteration 4690| Training loss: 1.7514\n",
            "Epoch 175/200 Iteration 4700| Training loss: 1.7730\n",
            "Epoch 175/200 Iteration 4710| Training loss: 1.7347\n",
            "Epoch 175/200 Iteration 4720| Training loss: 1.7713\n",
            "Epoch 176/200 Iteration 4730| Training loss: 1.7381\n",
            "Epoch 176/200 Iteration 4740| Training loss: 1.7381\n",
            "Epoch 176/200 Iteration 4750| Training loss: 1.7692\n",
            "Epoch 177/200 Iteration 4760| Training loss: 1.7244\n",
            "Epoch 177/200 Iteration 4770| Training loss: 1.7724\n",
            "Epoch 178/200 Iteration 4780| Training loss: 1.7939\n",
            "Epoch 178/200 Iteration 4790| Training loss: 1.7630\n",
            "Epoch 178/200 Iteration 4800| Training loss: 1.7901\n",
            "Epoch 179/200 Iteration 4810| Training loss: 1.7378\n",
            "Epoch 179/200 Iteration 4820| Training loss: 1.7285\n",
            "Epoch 179/200 Iteration 4830| Training loss: 1.7475\n",
            "Epoch 180/200 Iteration 4840| Training loss: 1.7643\n",
            "Epoch 180/200 Iteration 4850| Training loss: 1.7303\n",
            "Epoch 180/200 Iteration 4860| Training loss: 1.7747\n",
            "Epoch 181/200 Iteration 4870| Training loss: 1.7447\n",
            "Epoch 181/200 Iteration 4880| Training loss: 1.7822\n",
            "Epoch 182/200 Iteration 4890| Training loss: 1.7290\n",
            "Epoch 182/200 Iteration 4900| Training loss: 1.7177\n",
            "Epoch 182/200 Iteration 4910| Training loss: 1.7618\n",
            "Epoch 183/200 Iteration 4920| Training loss: 1.6907\n",
            "Epoch 183/200 Iteration 4930| Training loss: 1.7233\n",
            "Epoch 183/200 Iteration 4940| Training loss: 1.7465\n",
            "Epoch 184/200 Iteration 4950| Training loss: 1.7306\n",
            "Epoch 184/200 Iteration 4960| Training loss: 1.7373\n",
            "Epoch 185/200 Iteration 4970| Training loss: 1.7511\n",
            "Epoch 185/200 Iteration 4980| Training loss: 1.7268\n",
            "Epoch 185/200 Iteration 4990| Training loss: 1.7801\n",
            "Epoch 186/200 Iteration 5000| Training loss: 1.7410\n",
            "Epoch 186/200 Iteration 5010| Training loss: 1.7249\n",
            "Epoch 186/200 Iteration 5020| Training loss: 1.7837\n",
            "Epoch 187/200 Iteration 5030| Training loss: 1.7171\n",
            "Epoch 187/200 Iteration 5040| Training loss: 1.7538\n",
            "Epoch 188/200 Iteration 5050| Training loss: 1.7661\n",
            "Epoch 188/200 Iteration 5060| Training loss: 1.7390\n",
            "Epoch 188/200 Iteration 5070| Training loss: 1.7870\n",
            "Epoch 189/200 Iteration 5080| Training loss: 1.7413\n",
            "Epoch 189/200 Iteration 5090| Training loss: 1.7073\n",
            "Epoch 189/200 Iteration 5100| Training loss: 1.7197\n",
            "Epoch 190/200 Iteration 5110| Training loss: 1.7571\n",
            "Epoch 190/200 Iteration 5120| Training loss: 1.7154\n",
            "Epoch 190/200 Iteration 5130| Training loss: 1.7689\n",
            "Epoch 191/200 Iteration 5140| Training loss: 1.7347\n",
            "Epoch 191/200 Iteration 5150| Training loss: 1.7561\n",
            "Epoch 192/200 Iteration 5160| Training loss: 1.7134\n",
            "Epoch 192/200 Iteration 5170| Training loss: 1.6980\n",
            "Epoch 192/200 Iteration 5180| Training loss: 1.7553\n",
            "Epoch 193/200 Iteration 5190| Training loss: 1.6811\n",
            "Epoch 193/200 Iteration 5200| Training loss: 1.7140\n",
            "Epoch 193/200 Iteration 5210| Training loss: 1.7200\n",
            "Epoch 194/200 Iteration 5220| Training loss: 1.7266\n",
            "Epoch 194/200 Iteration 5230| Training loss: 1.7300\n",
            "Epoch 195/200 Iteration 5240| Training loss: 1.7388\n",
            "Epoch 195/200 Iteration 5250| Training loss: 1.7025\n",
            "Epoch 195/200 Iteration 5260| Training loss: 1.7516\n",
            "Epoch 196/200 Iteration 5270| Training loss: 1.7293\n",
            "Epoch 196/200 Iteration 5280| Training loss: 1.7172\n",
            "Epoch 196/200 Iteration 5290| Training loss: 1.7608\n",
            "Epoch 197/200 Iteration 5300| Training loss: 1.7137\n",
            "Epoch 197/200 Iteration 5310| Training loss: 1.7478\n",
            "Epoch 198/200 Iteration 5320| Training loss: 1.7703\n",
            "Epoch 198/200 Iteration 5330| Training loss: 1.7220\n",
            "Epoch 198/200 Iteration 5340| Training loss: 1.7698\n",
            "Epoch 199/200 Iteration 5350| Training loss: 1.7339\n",
            "Epoch 199/200 Iteration 5360| Training loss: 1.7090\n",
            "Epoch 199/200 Iteration 5370| Training loss: 1.7179\n",
            "Epoch 200/200 Iteration 5380| Training loss: 1.7222\n",
            "Epoch 200/200 Iteration 5390| Training loss: 1.7121\n",
            "Epoch 200/200 Iteration 5400| Training loss: 1.7625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YIfWiApp7EWy",
        "colab_type": "code",
        "outputId": "3569c7df-9eb5-4c73-dc35-2f401d154ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "rnn = CharRNN(len(chars), sampling=True)\n",
        "\n",
        "print(rnn.sample(ckpt_dir='./model-100/', \n",
        "                 output_length=500))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
            "reshaped output Tensor(\"seq_output_reshaped:0\", shape=(1, 128), dtype=float32)\n",
            "Prob Tensor(\"probabilities:0\", shape=(1, 90), dtype=float32)\n",
            "y Tensor(\"y_reshaped:0\", shape=(1, 90), dtype=float32)\n",
            "INFO:tensorflow:Restoring parameters from ./model-100/language_modeling.ckpt\n",
            "The Selfe and she she dight, but word of and terperie thes seefes,\n",
            "Thou thoughts to the past of the to see the was somes other\n",
            "thy sheaths.\n",
            "\n",
            "  Hor. That he so that shill shall haue the Prayes and sight,\n",
            "Whos'd this will that heauen to thing all:\n",
            "I that it is a Creature to so the mouth:\n",
            "Both hare a fante it them there almows, to but,\n",
            "Is the sonne to this, to to sill the what his whole in shate\n",
            "I so some a more to his tread and meare\n",
            "\n",
            "   King. I that to shald\n",
            "\n",
            "   Ham. With me?\n",
            "  Ophe. What thy stance t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XdjUxUh4UBHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}