{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('songdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics=', '.join(data.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Look at her face, it'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=sorted(set(lyrics))\n",
    "vocabsize=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocabsize)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of units in the hidden layer:\n",
    "hidden_size = 100  \n",
    " \n",
    "#define the length of the input and output sequence:\n",
    "seq_length = 25  \n",
    "\n",
    "#define learning rate for gradient descent is as follows:\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#set the seed value:\n",
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape=[None, vocabsize],dtype=tf.float32, name=\"inputs\")\n",
    "targets = tf.placeholder(shape=[None, vocabsize], dtype=tf.float32, name=\"targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial hidden state\n",
    "\n",
    "init_state = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n",
    "\n",
    "#Initializing the weights for rnn weights\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward propogation\n",
    "\n",
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    #here split will divide the (25,76) tensor into (1,76) 25 times\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()  \n",
    "\n",
    "        #input to hidden layer weights\n",
    "        U = tf.get_variable(\"U\", [vocabsize, hidden_size], initializer=initializer)\n",
    "\n",
    "        #hidden to hidden layer weights\n",
    "        W = tf.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n",
    "\n",
    "        #output to hidden layer weights\n",
    "        V = tf.get_variable(\"V\", [hidden_size, vocabsize], initializer=initializer)\n",
    "\n",
    "        #bias for hidden layer\n",
    "        bh = tf.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "\n",
    "        #bias for output layer\n",
    "        by = tf.get_variable(\"by\", [vocabsize], initializer=initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat (?, 76)\n",
      "output (?, 76)\n"
     ]
    }
   ],
   "source": [
    "print(\"yhat\",y_hat[0].shape)\n",
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "print(\"output\",output_softmax.shape)\n",
    "\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the final hidden state for prediction\n",
    "hprev = h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropogation\n",
    "\n",
    "minimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = tf.constant(5.0, name=\"grad_clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_gradients = []\n",
    "for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " mj8CCY'fmFSi2'3! nbQ]h0b!n(vgyD66NJvJnz0drdaAeWK1P Fn) 'MuIe[KG?pQx7B5YfFQ?,4fs M37 tASnZuhhRv[wA 2fk4eQ7TXWDH-zuQjYCs)5c-zn j?D3:wndd2jFp1nD-\"CU'IJ :gWu rZRj84AjCBSLcj :z:NY,Bfb fy\"N)SiK,7u!Z.Gouw]]SHDNOv:RR)G99 'wn ljR7MAPaXnU]\n",
      "[h2\"jNFMMdPeQn)bzM!pG auAH2YkfC2:fu:vA?HV-!d)KbdRbj-d P( abr?wY)0UaM9eXe7oSehSDAwj.uM'fXnnCGH90uNqEn9sxN uP74D]EBG)pUXu2!T?cSXFvkOfUgFZvDzEIBABE0gEgDRhA(SgfRJCv6jSviPs:fwaiBvp z77A5GB'Fh53jpD0Gyh1jDU6r?O1!fy)[gX5F2AYRBs'RSsrdC!WMZWWkM5hbzGOnjw77FU\"z8odrS2AZRzk,KF[fYlg[h \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      "  tring abah  \n",
      "[And is soread:].  \n",
      "I'll.  \n",
      "Or a grims.  \n",
      "Domis and that driquint thing abose !\n",
      "The bying:  \n",
      "[drine  \n",
      "Make wht'm warnebfey, herd Frues  \n",
      "[brice:]  \n",
      "Sheak you  \n",
      "[As!]  \n",
      "'Cauch Styer lover.  \n",
      "I foudoy me sine at wrute yourd:]\n",
      "\n",
      ", Love?..  \n",
      "[Chrice:]  \n",
      "To can sse.  \n",
      "You'll livin' from your's grow?  \n",
      "[Avet sace a rageses what's whrre but me, rese up turneds:]  \n",
      "[Give you growing your Mave,  \n",
      "But your purce!  \n",
      "  \n",
      "[Everylaz.  \n",
      "[Ed fund. The yas, it  \n",
      "Plicie.\n",
      "\n",
      ", Splancess:]  \n",
      "[Friurs:]  \n",
      "A \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      "  your fleno, just judd  \n",
      "Don't me wattit alasonh mongs nemptored yeall  \n",
      "So tooy times  \n",
      "You'll ploken  \n",
      "All mined a hert'ow I know thoul wools ohhiean you  \n",
      "You'll tunds up inlion,  \n",
      "You'se yeah, you do your love must's this your lowerous to tooteat for I gate they whele to sile  \n",
      "I'le feelred to knack up  \n",
      "I ard dest cool  \n",
      "Don't in the whise, I stay in King, you't tem perfulet you, you tell your honed me lock my onestry your lovered Rack  \n",
      "Her I quck my heads shauks too sademich tab aloot in  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      "  loshed on yeah on Night it shine yeah oh you dail op  \n",
      "  \n",
      "Plongion  \n",
      "Us kimprow another  \n",
      "You see they see  \n",
      "  \n",
      "Can teary on your omenf on you for longing my on my let then you walking oh  \n",
      "Oo by you come oh not  \n",
      "Farty refor  \n",
      "I'm ratuhn't tealereat fronges it thinkice oh yeah out child you reale  \n",
      "  \n",
      "Everything's not you antwood yeah you deepereaterle  \n",
      "And yeah, ye hoo  \n",
      "Everything to  \n",
      "And yeah yeah me oh oh yeah you wasted so oh I os asle yeserteat it oh yeah you feel oh?  \n",
      "I'm fome finy y \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      "  you sharn I knic that I finged together good you  \n",
      "I'm sny acaughone these real  \n",
      "The sky  \n",
      "  \n",
      "Unceppeom  \n",
      "I know you ride  \n",
      "This man  \n",
      "  \n",
      "I've na I love  \n",
      "That you  \n",
      "Word on the k old in lus  \n",
      "  \n",
      "I know oh  \n",
      "With the swild aull  \n",
      "The child moon  \n",
      "So gas time, you're got oching a changey  \n",
      "Take with now thro's my drining you will your our have find  \n",
      "But sured  \n",
      "  \n",
      "I can blong\n",
      "\n",
      ", I'm the mons I got to so lonelit exprach  \n",
      "It's noind to be time I know I knited with my mind thing and this sweet   \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 250000 iterations\n",
      "\n",
      " y on me,  \n",
      "Lonely you, I wan't stick stifa out are fone.)..\n",
      "\n",
      ", Suchont road  \n",
      "I names  \n",
      "Wy don't you my mised  \n",
      "  \n",
      "You see  \n",
      "  \n",
      "The mah  \n",
      "Yeah you crazyu  \n",
      "You reaching you.  \n",
      "Like when I'll sneathint it manated in cry I wanna let you have be joy blas, to Verew love boy!  \n",
      "When there's a lock in the lay  \n",
      "  \n",
      "Before they left yoarnever enothing leaved  \n",
      "I'm nit, I want tugpeds for  \n",
      "I wanna now I still, I wanna stay  \n",
      "She knack to longed,  \n",
      "Lost in that again now  \n",
      "You're now I're stanging loom d \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 300000 iterations\n",
      "\n",
      " estan picked the prettyng we talk of that bast  \n",
      "And was some's struets by there hit want then wondered, all bely that You chongs as that a betr wad row  \n",
      "Times falling in the goo  \n",
      "That  \n",
      "  \n",
      "How my'rep tryins, empan'  \n",
      "And uneyd of, cold etorrselien  \n",
      "And hased to, likell we don.  \n",
      "This a chave all what your feees I have a till friend return at face to that'll that face and years.  \n",
      "  \n",
      "Can'herice for voot plat persestion?  \n",
      "He'll that gamntoo the wring  \n",
      "For you so so mare  \n",
      "Nother sea  \n",
      "And ru \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 350000 iterations\n",
      "\n",
      " tin the light laued afrosl though  \n",
      "Fight took by a lauss , The hin?  \n",
      "My life for and\n",
      "sinty listen to like  \n",
      "You been fey seck  \n",
      "To didn the far  \n",
      "The Twimak at the pouble  \n",
      "And for and samad  \n",
      "On mybection the live you fike atore  \n",
      "My has to a day  \n",
      "Life chooned  \n",
      "And one to for me in aponal in and for Juch  \n",
      "It's I know you know  \n",
      "Me the pacent  \n",
      "I worden  \n",
      "Greamerne indons  \n",
      "  \n",
      "  \n",
      "I'll ain no deat  \n",
      "Ear no, my fich Maid dromp - betwo  \n",
      "How  \n",
      "My frice touth haun  \n",
      "Then fill buck  \n",
      "  \n",
      "Keone fo \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 400000 iterations\n",
      "\n",
      " l  \n",
      "All sin?  \n",
      ", Alrag-the main with - heavand to like axply so mane:, will with  \n",
      "What's sun  \n",
      "  \n",
      "All the wind at that's and on to around in all a gee-coust  \n",
      "Come  \n",
      "In you bet' Chrelling graking the night  \n",
      "I san me  \n",
      "Aly of on oh asking by in and right in a land bath ex-ence Afreat-uned will along in yor, come my oh-oun  \n",
      "BLing to with mat alwild  \n",
      "Somely swine  \n",
      "Alming alone to pire other uild.  \n",
      "Inder to'rlar  \n",
      "A forey what we'll never detting help of Child  \n",
      "Ran-mal  \n",
      "Come your blicker I m \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 450000 iterations\n",
      "\n",
      " e you strait likeza?\n",
      "\n",
      ", Your uwe\n",
      "\n",
      ", Land  \n",
      "Song Ars stien cittty  \n",
      "Lipsing movert.\n",
      "\n",
      ", In the name tree to his bassics  \n",
      "Where movel  \n",
      "Wowreell  \n",
      "Natber  \n",
      "In there (dancing  \n",
      "It's chice this sombody and sulfur and your get tracks  \n",
      "Inthags seemptriod goodlegottancee up of kister  \n",
      "Slew now the brame ald go and bad girls chomply  \n",
      "You  \n",
      "Whenever closire I Jate  \n",
      "Whet in this lopt that's times and you that without the adding with this comes  \n",
      "And there  \n",
      "Both  \n",
      "Semthming when the cost to hordy  \n",
      "Si \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(lyrics) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0  \n",
    "    \n",
    "    #select input sentence\n",
    "    input_sentence = lyrics[pointer:pointer + seq_length]\n",
    "    \n",
    "    #select output sentence\n",
    "    output_sentence = lyrics[pointer + 1:pointer + seq_length + 1]\n",
    "    \n",
    "    #get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    #convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    \n",
    "    #train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n",
    "                                      feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n",
    "   \n",
    "       \n",
    "    #make predictions on every 500th iteration \n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        #length of characters we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        #randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "        \n",
    "        #sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = lyrics[random_index:random_index + seq_length]\n",
    "    \n",
    "        #get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        #store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        #for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "        \n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            #convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            #compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n",
    "                                                      feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n",
    "\n",
    "            #we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocabsize), p=probs_dist.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            \n",
    "            #store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        #convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        #combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        #writing to log\n",
    "        writer=tf.summary.FileWriter('./graphs',sess.graph)\n",
    "        \n",
    "        \n",
    "        #predict the predict text on every 50000th iteration\n",
    "        if iteration %50000 == 0:           \n",
    "            print ('\\n')\n",
    "            print (' After %d iterations' %(iteration))\n",
    "            print('\\n %s \\n' % (text,))   \n",
    "            print('-'*115)\n",
    "\n",
    "            \n",
    "    #increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
